<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Research | Pan ZHOU </title> <meta name="author" content="Pan ZHOU"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/log.ico?cada1a831dbcff16e2bae7854c63d30d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://panzhou916.github.io/research/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <b>Pan ZHOU</b> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> <b>Home</b> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/"> <b>Research</b> <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publication/"> <b>Publication</b> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/"> <b>Teaching</b> </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/"> <b>Service</b> </a> </li> <li class="nav-item "> <a class="nav-link" href="/recruitment/"> <b>Recruitment</b> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Research</h1> <p class="post-description"></p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/framework-480.webp 480w,/assets/img/framework-800.webp 800w,/assets/img/framework-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/framework.png" width="100%" height="auto" title="research image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>My research target is to build <strong>”efficient and effective artificial intelligent systems”</strong> so that machines can cognize, understand and interact with the environment. Currently, I mainly focus on three research topics across machine learning, computer vision and optimization.</p> <p><strong>1) Efficiency Optimization</strong>: improve the efficiency of training (including fine-tuning) and inference efficiency for AI models like LLMs.</p> <ul> <li> <strong>a) Faster Training Algorithms/Optimizers</strong>: design more effective and efficient optimizers to faster train AI models especially for LLMs. <ul> <li> <details> <summary> <strong>Faster Optimizer</strong> (click here for representative works)</summary> <strong><a href="https://arxiv.org/abs/2208.06677" rel="external nofollow noopener" target="_blank">Adan</a></strong> (TPAMI, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/Adan?style=social"> </iframe>) is about 2x faster than the SoTA optimizers, e.g. Adam, while achieving higher or comparable performance on many networks, e.g., CNNs, ViTs and MAE in the CV field, UNet and ViTs in AIGC field, GPT2 and billion-scale LLaMA in the NLP field, networks in RL tasks. It has been included in popular deep-learning codebases, e.g., <a href="https://github.com/NVIDIA/NeMo/blob/main/nemo/core/optim/adan.py" rel="external nofollow noopener" target="_blank">NVIDIA NeMo</a> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/NVIDIA/NeMo?style=social"> </iframe>) for training large language models and multi-modal models, <a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/optim/adan.py" rel="external nofollow noopener" target="_blank">HuggingFace Timm</a> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/huggingface/pytorch-image-models?style=social"> </iframe>) and <a href="https://github.com/open-mmlab/mmpretrain/blob/dev-1.x/mmcls/engine/optimizers/adan_t.py" rel="external nofollow noopener" target="_blank">OpenMMLab</a> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/open-mmlab/mmpretrain?style=social"> </iframe>) which both train AI models for CV tasks like classification, detection and segmentation, <a href="https://github.com/Jittor/jittor/blob/master/python/jittor/optim.py" rel="external nofollow noopener" target="_blank">Jittor of Tsinghua University</a> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/Jittor/jittor?style=social"> </iframe>) for 3D generation, and is the default optimizer in <a href="https://github.com/ashawkey/stable-dreamfusion/blob/main/optimizer.py" rel="external nofollow noopener" target="_blank">DreamFusion</a> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/ashawkey/stable-dreamfusion?style=social"> </iframe>) and <a href="https://arxiv.org/abs/2303.14389" rel="external nofollow noopener" target="_blank">MDT</a> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/MDT?style=social"> </iframe>) for SoTA 3D and image generation tasks respectively. <p>See more works, e.g., <strong><a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf" rel="external nofollow noopener" target="_blank">SLRLA</a></strong> (NeurIPS, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/SLRLA-optimizer?style=social"> </iframe>) which improves lookahead, <strong><a href="https://ieeexplore.ieee.org/document/8792163" rel="external nofollow noopener" target="_blank">R-SPIDER</a></strong> (TPAMI &amp; AISTATS), and <strong><a href="https://arxiv.org/pdf/2009.09835.pdf" rel="external nofollow noopener" target="_blank">HSDMPG</a></strong> (ICML &amp; TPAMI). </p> </details> </li> <li> <details> <summary> <strong>Plug-and-play Acceleration Framework</strong> (click here for representative works)</summary> <strong><a href="../assets/pdf/2024-JMLR-win.pdf">Win</a></strong> (JMLR &amp; ICLR, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/win?style=social"> </iframe>) can accelerate AdamW, Adam, LAMB and SGD by 1.5X on vision classification tasks and language modeling tasks with both CNN and Transformer backbones. </details> </li> <li> <details> <summary> <strong>Network Optimization Theory</strong> (click here for representative works)</summary> <strong><a href="https://arxiv.org/pdf/2010.05627.pdf" rel="external nofollow noopener" target="_blank">This work</a></strong> (NeurIPS, 200+ citations <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/salesforce/comparison_SGD_ADAM?style=social"> </iframe>) provides the first theory to explain "why SGD generalizes better than ADAM in deep learning". See more works like <strong><a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf" rel="external nofollow noopener" target="_blank">SLRLA</a></strong> (NeurIPS, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/SLRLA-optimizer?style=social"> </iframe>) to analyzes "why Lookahead generalizes better than SGD". </details> </li> </ul> </li> <li> <strong>b) Inference Acceleration of AI Models like LLMs</strong>: design faster inference techniques for AI models like LLMs and multimodal models. <ul> <li> <details> <summary> <strong>Inference Acceleration of (multimodal) LLMs</strong> (click here for representative works)</summary> <strong><a href="https://arxiv.org/pdf/2509.22134" rel="external nofollow noopener" target="_blank">GTO</a></strong> is the first one that aligns the training and inference decoding objectives in speculative decoding, and can accelerate the vanilla auto-regressive decoding by around 4 times. See more works like <strong><a href="https://arxiv.org/abs/2502.11018" rel="external nofollow noopener" target="_blank">GRIFFIN</a></strong> (NeurIPS). </details> </li> </ul> </li> </ul> <p><strong>2) Learning Framework</strong>: design effective learning framework / training task / loss to formulate a problem so that the AI models can learn desired knowledge to handle general / specific tasks.</p> <ul> <li> <strong>a) Self-Supervised (multi-modal) Learning</strong>: design effective and efficient self-supervised (multi-modal) learning frameworks that enable AI models to learn desired knowledge and achieve human’s data understanding and reasoning ability. <ul> <li> <details> <summary> <strong>Single-modal Learning</strong> (click here for representative works)</summary> <strong><a href="https://openreview.net/pdf?id=KmykpuSrjcq" rel="external nofollow noopener" target="_blank">PCL</a></strong> (ICLR, 800+ citations, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/salesforce/PCL?style=social"> </iframe>) is the first clustering contrastive learning method to learn data cluster structure, and its improved version, <strong><a href="https://arxiv.org/abs/2203.14415" rel="external nofollow noopener" target="_blank">Mugs</a></strong> (<iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/mugs?style=social"> </iframe>), develops multi-granular contrastive learning for multi-granular representation learning. See more works like <strong><a href="https://arxiv.org/pdf/2106.14749.pdf" rel="external nofollow noopener" target="_blank">SANE</a></strong> (NeurIPS, spotlight), <strong><a href="https://arxiv.org/abs/2210.11016" rel="external nofollow noopener" target="_blank">TEC</a></strong>, and <strong><a href="https://arxiv.org/pdf/2106.09645.pdf" rel="external nofollow noopener" target="_blank">PGCL</a></strong> (TNNLS). </details> </li> <li> <details> <summary> <strong>Multi-modal Learning</strong> (click here for representative works)</summary> <strong><a href="https://arxiv.org/pdf/2212.09737.pdf" rel="external nofollow noopener" target="_blank">PTP</a></strong> (TPAMI &amp; CVPR, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/ptp?style=social"> </iframe>) is the pioneer to enhance grounding ability in multi-modal models, and <strong><a href="https://arxiv.org/abs/2405.14974" rel="external nofollow noopener" target="_blank">LOVA<sup>3</sup></a></strong> is to empower models with humans' ability, including the answering, asking and accessing ability. See more works like <strong><a href="https://arxiv.org/abs/2312.02439" rel="external nofollow noopener" target="_blank">CLOT</a></strong>(CVPR, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/CLoT?style=social"> </iframe>) for exploring humans' creativity, <strong><a href="https://arxiv.org/abs/2302.13668" rel="external nofollow noopener" target="_blank">CoVGT</a></strong> (TPAMI &amp; ECCV, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/doc-doc/CoVGT?style=social"> </iframe>) for video question answering, <strong><a href="https://arxiv.org/abs/2312.06731" rel="external nofollow noopener" target="_blank">Genixer</a></strong> (ECCV) to empower multi-modal models as a powerful data generator, <strong><a href="https://arxiv.org/pdf/2109.09161.pdf" rel="external nofollow noopener" target="_blank">Wav-BERT</a></strong> (AAAI) for acoustic and linguistic representation learning. </details> </li> <li> <details> <summary> <strong>Agentic Learning</strong> (click here for representative works)</summary> <strong><a href="https://arxiv.org/abs/2411.04679" rel="external nofollow noopener" target="_blank">CaPo</a></strong> (ICLR) introduces a two-phase cooperation framework—meta-plan generation followed by progress-adaptive execution—enabling LLM-based agents to form coherent long-term plans and refine them dynamically via multi-turn discussions, eliminating redundant actions and improving efficiency. Then <strong><a href="https://panzhous.github.io/assets/pdf/2025-CVPR-CoTS.pdf" rel="external nofollow noopener" target="_blank">CoTS</a></strong> (CVPR) incorporates an LLM-driven Monte Carlo tree search and a plan-evaluation module that stabilizes updates and ensures timely plan revisions, further enhancing the scalability and reliability of multi-agent collaboration. </details> </li> </ul> </li> <li> <strong>b) Generative Models</strong>: design generative models like diffusion models that generate image/3D/video data and empower AI models with imagination and creativity akin to that of humans. <ul> <li> <details> <summary> <strong>Image Generation</strong> (click here for representative works)</summary> <strong><a href="https://arxiv.org/abs/2303.14389" rel="external nofollow noopener" target="_blank">MDT</a></strong> (ICCV, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="335px" height="20px" src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/masked-diffusion-transformer-is-a-strong/image-generation-on-imagenet-256x256"> </iframe>, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/MDT?style=social"> </iframe>) achieves SoTA image synthesis performance on ImageNet (256x256), and improves the learning speed of <a href="https://arxiv.org/abs/2212.09748" rel="external nofollow noopener" target="_blank">DiT</a> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/facebookresearch/DiT?style=social"> </iframe>), the core component of <a href="https://openai.com/sora" rel="external nofollow noopener" target="_blank">SORA</a>, by at least 10x. See more works like <strong><a href="https://github.com/sail-sg/EditAnything" rel="external nofollow noopener" target="_blank">EditAnything</a></strong> (ACMMM, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/EditAnything?style=social"> </iframe>), <strong><a href="https://arxiv.org/abs/2306.06991" rel="external nofollow noopener" target="_blank">FDT</a></strong> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/FDM?style=social"> </iframe>), <strong><a href="https://arxiv.org/pdf/2310.13545.pdf" rel="external nofollow noopener" target="_blank">ScaleLong</a></strong> (NeurIPS, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/ScaleLong?style=social"> </iframe>), and <strong><a href="https://arxiv.org/pdf/2006.06900.pdf" rel="external nofollow noopener" target="_blank">PPOGAN</a></strong> (NeurIPS, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/Holmeswww/PPOGAN?style=social"> </iframe>). </details> </li> <li> <details> <summary> <strong>3D Generation</strong> (click here for representative works)</summary> <strong><a href="https://arxiv.org/abs/2401.09050" rel="external nofollow noopener" target="_blank">Consistent3D</a></strong> (CVPR, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/Consistent3D?style=social"> </iframe>) is the pioneer to use ODE sampling as guidance in text-to-3D task, overcoming the unpredicable and unstable SDE guidance in <a href="https://arxiv.org/abs/2209.14988" rel="external nofollow noopener" target="_blank">SDS/DreamFusion</a>. See more works, e.g., <strong><a href="ttps://github.com/yxymessi/DTC123/blob/main/DTC_CVPR.pdf">DTC123</a></strong> (CVPR, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/yxymessi/DTC123?style=social"> </iframe>) and <strong><a href="https://arxiv.org/abs/2403.18795" rel="external nofollow noopener" target="_blank">Gamba</a></strong> for image-to-3D generation, <strong><a href="https://arxiv.org/abs/2311.08403" rel="external nofollow noopener" target="_blank">Instant3D</a></strong> (IJCV, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/ming1993li/Instant3DCodes?style=social"> </iframe>) for fast text-to-3D task. </details> </li> </ul> </li> </ul> <p></p> <p><strong>3) Network Architecture Design</strong>: develop innovative network topology that posses high capacity and efficiency for acquiring knowledge, thereby improving the overall model capacity of AI.</p> <ul> <li> <details> <summary> <strong>Manually-Designed Network</strong> (click here for representative works)</summary> <strong><a href="https://arxiv.org/abs/2111.11418#:~:text=Based%20on%20the%20extensive%20experiments,on%20the%20token%20mixer%20modules." rel="external nofollow noopener" target="_blank">MetaFormer</a></strong> (CVPR ORAL, 600+ citations, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/poolformer?style=social"> </iframe>) replaces self-attention in ViT with pooling and convolutions independently, and achieves impressive performance, breaking the slogan “self-attention is all you need”. It reveals network design princeples that if a network contains two kinds of operations, including spatial information exchanging operations (e.g., attention, pooling and convolution) and channel information exchanging operations (e.g., MLP), then the network can perfor well. Its improved version CAFormer network sets a new recording accuracy of 85.5% on ImageNet under supervised settings without extra training data, and achives top-2 performance on ImageNet-C( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="400px" height="20px" src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/metaformer-baselines-for-vision/domain-generalization-on-imagenet-c"> </iframe>). See more works like <strong><a href="https://arxiv.org/abs/2205.12956" rel="external nofollow noopener" target="_blank">IFormer</a></strong> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/iFormer?style=social"> </iframe>), <strong><a href="https://arxiv.org/abs/2303.16900" rel="external nofollow noopener" target="_blank">InceptionNeXt</a></strong> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/inceptionnext?style=social"> </iframe>), <strong><a href="https://arxiv.org/abs/2112.04674" rel="external nofollow noopener" target="_blank">DualFormer</a></strong> ( <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/sail-sg/dualformer?style=social"> </iframe>), and <strong><a href="https://arxiv.org/abs/2203.07057" rel="external nofollow noopener" target="_blank">SUN</a></strong> (ECCV, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/DongSky/few-shot-vit?style=social"> </iframe>). </details> </li> <li> <details> <summary> <strong>Automatically-Designed Network</strong> (click here for representative works)</summary> <strong><a href="https://arxiv.org/abs/2006.16537" rel="external nofollow noopener" target="_blank">PR-DARTS</a></strong> (NeurIPS ORAL, <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://img.shields.io/github/stars/salesforce/PR-DARTS?style=social"> </iframe>) automatically designs effective network architectures, reducing the reliance on expert trial and error. It provides the first theory to show why previous network search methods (a.k.a. AutoML) often collapse due to selecting too many skip-connections, and then proposes a new method that can avoid previous collapse and thus automatically selects and combines various network operations, e.g. pooling and convolution, to search more effective network. </details> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Pan ZHOU. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>